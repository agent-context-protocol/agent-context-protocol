import os
import subprocess

def create_pdf_from_latex(latex_code, output_pdf_path):
    # Ensure the output directory exists
    output_dir = os.path.dirname(output_pdf_path)
    os.makedirs(output_dir, exist_ok=True)

    # Step 1: Write the LaTeX code to a .tex file inside the desired output directory
    tex_file_name = "temp_latex_file.tex"  # Keep the filename consistent
    tex_file_path = os.path.join(output_dir, tex_file_name)
    with open(tex_file_path, "w") as tex_file:
        tex_file.write(latex_code)

    try:
        # Step 2: Compile the .tex file into a PDF using pdflatex with -output-directory
        for _ in range(2):  # Run twice to resolve references
            result = subprocess.run(
                [
                    "pdflatex",
                    "-interaction=nonstopmode",
                    f"-output-directory={output_dir}",
                    tex_file_path,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            print(result.stdout)  # Print compilation log for debugging

        # Step 3: Verify if the generated PDF exists
        generated_pdf = os.path.join(output_dir, tex_file_name.replace(".tex", ".pdf"))
        if os.path.exists(generated_pdf):
            # Move the generated PDF to the desired output path
            os.rename(generated_pdf, output_pdf_path)
            print(f"PDF generated successfully at: {output_pdf_path}")
        else:
            print(f"PDF generation failed. File not found: {generated_pdf}")

    except subprocess.CalledProcessError as e:
        print("Error during PDF generation. Please check the LaTeX source.")
        print(e.stdout)
        print(e.stderr)

    finally:
        # Step 4: Clean up auxiliary files generated by LaTeX in the save folder
        for ext in [".aux", ".log", ".out", ".tex"]:
            aux_file = os.path.join(output_dir, tex_file_name.replace(".tex", ext))
            if os.path.exists(aux_file):
                os.remove(aux_file)

# Example Usage
latex_code = r"""
\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, graphicx, booktabs, caption, url, hyperref, longtable, placeins}
\usepackage{titling}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{array}
\usepackage{float} % Provides more float placement control if needed

% Styling adjustments
\allsectionsfont{\sffamily\bfseries}
\setstretch{1.1}
\captionsetup{font={small,it},justification=centering}
\setlength{\droptitle}{-2cm} % Reducing space above the title
\titlespacing*{\section}{0pt}{*1}{*0.5} % Adjusting space before and after sections

% Custom column types for longtable
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}} % Left-aligned column with fixed width
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}   % Centered column with fixed width
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}  % Right-aligned column with fixed width

% Hyperref settings for long URLs
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    breaklinks=true
}

\title{The Evolution of Language Models and Their Impact on the AI Landscape}
\author{}
\date{}

\begin{document}
\maketitle

\section{Section 1: Historical Evolution and Technical Advancements}

\subsection{Subsection 1.1: Historical Evolution: From N-Grams to Neural Networks}
The historical evolution of language models from N-Grams to Neural Networks and the introduction of Transformer Models is a fascinating journey that highlights significant advancements in natural language processing (NLP) and machine learning. This evolution can be broken down into several key phases, each marked by technical innovations and improvements in scalability.

\paragraph{Early Language Models: N-Grams and Statistical Approaches}
N-Grams and Hidden Markov Models (HMMs) were among the earliest statistical models used in NLP. N-Grams are probabilistic models that predict the likelihood of a word based on the preceding N-1 words. HMMs were used extensively in the 1990s for tasks like speech recognition and part-of-speech tagging. However, these models struggled with capturing long-range dependencies due to their reliance on fixed-size context windows.

\paragraph{Transition to Neural Networks}
The early 2000s saw the introduction of neural networks for language modeling. Models such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs) were capable of handling sequential data and capturing dependencies over longer contexts. The advent of Graphics Processing Units (GPUs) in 1999 enabled the efficient processing of large datasets, significantly enhancing the scalability of neural network models.

\subsection{Subsection 1.2: Introduction of Transformer Models}
The introduction of the Transformer model in 2017 by Vaswani et al. marked a paradigm shift in language modeling. It utilized a self-attention mechanism, allowing for parallel processing and the capture of long-range dependencies without the need for recurrent connections. Transformers have become the foundation for state-of-the-art models like BERT, GPT, and T5, which excel in various NLP tasks.

\subsection{Subsection 1.3: Technical Advancements: Architecture and Scalability}
The timeline chart captures the chronological progression of language models, highlighting key innovations and technical advancements. The comparative table below uses `longtable` to ensure that if the table is very long, it can continue onto the next page naturally without awkward shifts or cutting off the content.

\renewcommand{\arraystretch}{1.5} % Increase row height in tables

\begin{longtable}{|L{4cm}|L{3.5cm}|L{3.5cm}|L{4cm}|}
\caption{\textit{Performance enhancements and scalability improvements across various model architectures.}} \\
\hline
\textbf{Model Type} & \textbf{Key Innovations/Features} & \textbf{Parameter Size (if applicable)} & \textbf{Scalability and Performance Enhancements} \\ \hline
\endfirsthead
\caption*{\textit{(Continued)}}\\
\hline
\textbf{Model Type} & \textbf{Key Innovations/Features} & \textbf{Parameter Size (if applicable)} & \textbf{Scalability and Performance Enhancements} \\ \hline
\endhead

N-Grams & Short-range dependencies & N/A & Simple probabilistic models, limited context \\ \hline
Neural Networks & RNNs, LSTMs, long-range dependencies & N/A & Improved handling of sequential data \\ \hline
Word Embeddings & Vector representation of words & N/A & Enhanced NLP tasks through dense representations \\ \hline
Transformers & Parallel processing, attention mechanism & N/A & Revolutionized language modeling, extensive dependencies \\ \hline
GPT-1 & Generative pre-trained transformer & 117 million & Initial large-scale language model \\ \hline
GPT-2 & Improved language understanding and generation & 1.5 billion & Significant increase in model size and capabilities \\ \hline
GPT-3 & Massive model size, enhanced capabilities & 175 billion & Substantial leap in language processing capabilities \\ \hline
LaMDA, Turing NLG, Bard & Large language models based on Transformers & 137-175 billion & Further advancements in scalability and performance \\ \hline
LaMDA, Turing NLG, Bard & Large language models based on Transformers & 137-175 billion & Further advancements in scalability and performance \\ \hline
LaMDA, Turing NLG, Bard & Large language models based on Transformers & 137-175 billion & Further advancements in scalability and performance \\ \hline
LaMDA, Turing NLG, Bard & Large language models based on Transformers & 137-175 billion & Further advancements in scalability and performance \\ \hline

\end{longtable}

% We are not placing a FloatBarrier immediately after the table to allow LaTeX to place other floats more optimally.
% Instead, we can place a FloatBarrier later if needed, e.g., at the end of the section.

\section{Section 2: Mathematical Foundations and Impact on AI Landscape}

\subsection{Subsection 2.1: Mathematical Foundations: Description and Optimization}

Transformer models have become central to modern NLP applications due to their ability to handle long-range dependencies in text. The architecture of a transformer model consists of an encoder and a decoder, both of which are composed of layers that include self-attention and feed-forward neural networks.

\paragraph{Optimization Techniques}
Optimization techniques are crucial for improving the efficiency and performance of transformer models. These include:

\begin{itemize}
    \item \textbf{Quantization:} Reduces the precision of the model's weights, significantly reducing memory footprint and speeding up computation.
    \item \textbf{Pruning:} Involves removing less significant weights or connections, reducing model size and improving inference speed and energy consumption.
    \item \textbf{Knowledge Distillation:} Involves training a smaller model to mimic a larger model, achieving similar performance with reduced size and complexity.
\end{itemize}

The impact of these optimization techniques is summarized in the following table. We continue to use standard table environments for shorter tables. If it doesn't fit, you can switch to longtable again. For this small table, it's likely fine.

\begin{longtable}{|L{4cm}|C{3cm}|C{3cm}|C{3cm}|}
\caption{\textit{Fossil fuel consumption trends (2005â€“2010).}}\label{tab:fossil_consumption}\\
\hline
\textbf{Year} & \textbf{Coal Consumption (EJ)} & \textbf{Oil Consumption (EJ)} & \textbf{Natural Gas Consumption (EJ)} \\
\hline
\endfirsthead
\caption*{\textit{(Continued)}}\\
\hline
\textbf{Year} & \textbf{Coal Consumption (EJ)} & \textbf{Oil Consumption (EJ)} & \textbf{Natural Gas Consumption (EJ)} \\
\hline
\endhead

2005 & 130 & 160 & 90 \\
\hline
2010 & 150 & 175 & 100 \\
\hline
\end{longtable}


% Not placing \FloatBarrier immediately after the table; let LaTeX decide.
% If floats still misbehave, we can place a single \FloatBarrier at the end of the subsection or section.

\subsection{Subsection 2.2: Applications and Future Directions}

Transformer models have enabled significant advancements in understanding and generating human language, leading to more sophisticated and capable AI systems. These models have been instrumental in applications ranging from machine translation and sentiment analysis to chatbots and virtual assistants.

% If we absolutely need to prevent any further floats from crossing into the references, we can place a FloatBarrier here:
%\FloatBarrier

\section{References}

\subsection*{Section 1: Historical Evolution and Technical Advancements}
\begin{itemize}
    \item \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4625356}{The Evolution of Language Models: From N-Grams to LLMs, and Beyond}
    \item \href{https://medium.com/@adria.cabello/the-evolution-of-language-models-a-journey-through-time-3179f72ae7eb}{The Evolution of Language Models: A Journey Through Time}
    \item \href{https://www.geeksforgeeks.org/the-evolution-of-language-models-from-gpt-1-to-gpt-4-and-beyond/}{The Evolution of Language Models: From GPT-1 to GPT-4 and Beyond}
    \item \href{https://medium.com/@sandeepsign/a-survey-evolution-of-language-models-2e3ec9990be2}{A Survey: Evolution of Language Models}
\end{itemize}

\subsection*{Section 2: Mathematical Foundations and Impact on AI Landscape}
\begin{itemize}
    \item \href{https://www.restack.io/p/large-language-models-answer-mathematical-foundations-cat-ai}{Mathematical Foundations of Large Language Models}
    \item \href{https://medium.com/@ssrivastav_46455/unveiling-the-mathematics-behind-transformer-models-shaurya-srivastav-2fe1e47d20f9}{Unveiling the Mathematics Behind Transformer Models}
    \item \href{https://arxiv.org/pdf/2408.03130}{Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations}
    \item \href{https://www.geeksforgeeks.org/architecture-and-working-of-transformers-in-deep-learning/}{Architecture and Working of Transformers in Deep Learning}
    \item \href{https://medium.com/openvino-toolkit/joint-pruning-quantization-and-distillation-for-efficient-inference-of-transformers-21333481f2ad}{Joint Pruning, Quantization and Distillation for Efficient Inference of Transformers}
    \item \href{https://medium.com/@deep.bbd/part-3-model-optimization-techniques-a-deep-dive-into-quantization-pruning-and-knowledge-ef4a8c4d6d8b}{Part 3 â€” Model Optimization Techniques: A Deep Dive into Quantization, Pruning, and Knowledge Distillation}
\end{itemize}

\end{document}

"""

create_pdf_from_latex(latex_code, "save_reports/output_report.pdf")
